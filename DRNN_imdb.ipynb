{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying DRNN to IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = 3\n",
    "VOCAB_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(index_from = INDEX_FROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFHFJREFUeJzt3W9sFfed7/H3N8Z/hNPlr4uiONxEEVqZtXSTyk0jLQ+ue6Uk5EnYJ22caosAhYsULPaShGTjB+ndFWiFtKyo1Q3NCrdBWhxF2l2KNsnSCFmqULe7cW6jlOBbBXWhmPAvgbSRkbHBv/vAAzVJCJ5j47E975d0dOZ8z8w53yMlfDzzm/lNpJSQJJXPbUU3IEkqhgEgSSVlAEhSSRkAklRSBoAklZQBIEklZQBIUkkZAJJUUgaAJJXUnKIb+DKLFy9Od999d9FtSNKM8s4773yUUmq42XrTOgDuvvtuent7i25DkmaUiDg+nvU8BCRJJWUASFJJGQCSVFIGgCSV1E0DICLuioieiDgSEe9HxKas/r2IOBkR72aPR8ds85cRcTQifh0RD4+pP5LVjkbE87fmJ0mSxmM8ewCXgadTSsuBB4GnImJ59t7fpZTuyx5vAGTvPQ78CfAI8PcRURURVcAPgJXAcqBtzOdIM0Z3dzfNzc1UVVXR3NxMd3d30S1JFbnpaaAppVPAqWz504joA+78kk0eA15NKV0C/isijgIPZO8dTSn9BiAiXs3WPTKB/qUp1d3dTUdHB7t372bFihUcOnSIdevWAdDW1lZwd1I+ucYAIuJu4H7gP7LSxoh4LyK6ImJBVrsTODFms/6sdqO6NGNs3bqV3bt309raSnV1Na2trezevZutW7cW3ZqU27gDICJuB/4J+IuU0u+Bl4B7gfsY3UP428loKCLWR0RvRPSeO3duMj5SmjR9fX2sWLHiutqKFSvo6+srqCOpcuMKgIioZvQf/39MKf0zQErpTErpSkppBPgH/nCY5yRw15jNG7PajerXSSm9nFJqSSm1NDTc9EpmaUo1NTVx6NCh62qHDh2iqampoI6kyo3nLKAAdgN9KaUdY+p3jFntz4DD2fJ+4PGIqI2Ie4BlwH8CbwPLIuKeiKhhdKB4/+T8DGlqdHR0sG7dOnp6ehgeHqanp4d169bR0dFRdGtSbuOZC+hPgT8HfhUR72a1Fxg9i+c+IAHHgP8FkFJ6PyJeY3Rw9zLwVErpCkBEbAQOAFVAV0rp/Un8LdItd3Wgt729nb6+Ppqamti6dasDwJqRIqVUdA831NLSkpwMTpLyiYh3UkotN1vPK4ElqaQMAEkqKQNAkkrKAJCkkjIAJKmkDABJKikDQMrJ2UA1W0zrm8JL042zgWo28UIwKYfm5mY6OztpbW29Vuvp6aG9vZ3Dhw9/yZbS1BnvhWAGgJRDVVUVg4ODVFdXX6sNDw9TV1fHlStXCuxM+gOvBJZuAWcD1WxiAEg5OBuoZhMHgaUcnA1Us4ljAJI0yzgGIEn6UgaAJJWUASBJJWUASFJJGQCSVFIGgCSVlAEgSSVlAEhSSRkAUk7eD0CzhQEg5dDd3c2mTZsYGBggpcTAwACbNm0yBDQjGQBSDlu2bKGqqoquri4uXbpEV1cXVVVVbNmypejWpNwMACmH/v5+9uzZQ2trK9XV1bS2trJnzx76+/uLbk3KzQCQpJIyAKQcGhsbWb169XX3A1i9ejWNjY1FtyblZgBIOWzfvp3Lly+zdu1a6urqWLt2LZcvX2b79u1FtyblZgBIObS1tbFz507q6+sBqK+vZ+fOnd4QRjOSN4SRpFlm0m4IExF3RURPRByJiPcjYlNWXxgRb0XEB9nzgqweEfH9iDgaEe9FxNfGfNbqbP0PImL1RH6gJGlixnMI6DLwdEppOfAg8FRELAeeBw6mlJYBB7PXACuBZdljPfASjAYG8CLwDeAB4MWroSFJmno3DYCU0qmU0v/Nlj8F+oA7gceAV7LVXgFWZcuPAXvSqF8A8yPiDuBh4K2U0vmU0gXgLeCRSf01kqRxyzUIHBF3A/cD/wEsSSmdyt46DSzJlu8ETozZrD+r3aguSSrAuAMgIm4H/gn4i5TS78e+l0ZHkidlNDki1kdEb0T0njt3bjI+UpL0BcYVABFRzeg//v+YUvrnrHwmO7RD9nw2q58E7hqzeWNWu1H9Oimll1NKLSmlloaGhjy/RZKUw3jOAgpgN9CXUtox5q39wNUzeVYDPxlT/252NtCDwO+yQ0UHgIciYkE2+PtQVpMkFWDOONb5U+DPgV9FxLtZ7QXgb4DXImIdcBz4VvbeG8CjwFHgIrAGIKV0PiL+Gng7W++vUkrnJ+VXSJJy80IwSZplJu1CMEnS7GQASFJJGQCSVFIGgJRTe3s7dXV1RAR1dXW0t7cX3ZJUEQNAyqG9vZ1du3axbds2BgYG2LZtG7t27TIENCN5FpCUQ11dHdu2bWPz5s3Xajt27OCFF15gcHCwwM6kPxjvWUAGgJRDRDAwMMDcuXOv1S5evEh9fT3T+f8llYungUq3QG1tLbt27bqutmvXLmprawvqSKrceK4ElpR58sknee655wDYsGEDu3bt4rnnnmPDhg0FdyblZwBIOXR2dgLwwgsv8PTTT1NbW8uGDRuu1aWZxDEASZplHAOQJH0pA0CSSsoAkHLq7u6mubmZqqoqmpub6e7uLrolqSIOAks5dHd309HRwe7du1mxYgWHDh1i3bp1ALS1tRXcnZSPg8BSDs3NzaxatYp9+/bR19dHU1PTtdeHDx8uuj0JGP8gsHsAUg5Hjhzh4sWLn9sDOHbsWNGtSbk5BiDlUFNTw8aNG2ltbaW6uprW1lY2btxITU1N0a1JuRkAUg5DQ0N0dnbS09PD8PAwPT09dHZ2MjQ0VHRrUm4eApJyWL58OatWraK9vf3aGMB3vvMd9u3bV3RrUm7uAUg5dHR0sHfvXjo7OxkcHKSzs5O9e/fS0dFRdGtSbu4BSDm0tbXx85//nJUrV3Lp0iVqa2t58sknPQVUM5J7AFIO3d3dvP7667z55psMDQ3x5ptv8vrrr3sxmGYkrwOQcmhubqazs5PW1tZrtZ6eHtrb270OQNOGdwSTboGqqioGBweprq6+VhseHqauro4rV64U2Jn0B84GKt0CTU1NHDp06LraoUOHaGpqKqgjqXIOAks5dHR08O1vf5v6+np++9vfsnTpUgYGBti5c2fRrUm5uQcgVWg6Hz6VxsMAkHLYunUr69evp76+noigvr6e9evXs3Xr1qJbk3LzEJCUw5EjRzhz5gy33347AAMDA/zwhz/k448/LrgzKT/3AKQcqqqqGBkZoauri8HBQbq6uhgZGaGqqqro1qTcbhoAEdEVEWcj4vCY2vci4mREvJs9Hh3z3l9GxNGI+HVEPDym/khWOxoRz0/+T5FuvcuXL39u5s+amhouX75cUEdS5cazB/Bj4JEvqP9dSum+7PEGQEQsBx4H/iTb5u8joioiqoAfACuB5UBbtq4046xZs4b29nbq6upob29nzZo1RbckVeSmYwAppZ9FxN3j/LzHgFdTSpeA/4qIo8AD2XtHU0q/AYiIV7N1j+TuWCpQY2MjP/rRj9i7d++1G8I88cQTNDY2Ft2alNtExgA2RsR72SGiBVntTuDEmHX6s9qN6p8TEesjojcies+dOzeB9qTJt337dq5cucLatWupra1l7dq1XLlyhe3btxfdmpRbpQHwEnAvcB9wCvjbyWoopfRySqklpdTS0NAwWR8rTYq2tjZ27tx53WmgO3fudDZQzUgVnQaaUjpzdTki/gH41+zlSeCuMas2ZjW+pC7NKG1tbf6Dr1mhoj2AiLhjzMs/A66eIbQfeDwiaiPiHmAZ8J/A28CyiLgnImoYHSjeX3nbkqSJGs9poN3AvwN/HBH9EbEO2B4Rv4qI94BW4H8DpJTeB15jdHD334CnUkpXUkqXgY3AAaAPeC1bV5pxuru7aW5upqqqiubmZu8FoBlrPGcBfdG+7u4vWX8r8Lnr4rNTRd/I1Z00zXR3d7Np0ybq6+tJKTEwMMCmTZsAPCykGccrgaUctmzZwtDQ0HW1oaEhtmzZUlBHUuUMACmH/v7+a7OARgQwOitof39/kW1JFTEApJzmzJlz3VxAc+Y4p6JmJgNAyumz9wHwvgCaqfzTRcppcHCQhx9+mOHhYaqrq90D0IzlHoCUw8KFCxkcHGTRokXcdtttLFq0iMHBQRYuXFh0a1Ju/uki5TB37lxGRkaoq6sjpURdXR3z5s1j7ty5Rbcm5eYegJTDhx9+SEtLC8ePHyelxPHjx2lpaeHDDz8sujUpNwNAymH+/PkcPHiQJUuWcNttt7FkyRIOHjzI/Pnzi25Nys0AkHL45JNPiAieffZZPv30U5599lkigk8++aTo1qTcDAAph5GREZ555hm6urr4yle+QldXF8888wwjIyNFtyblZgBIOS1evJjDhw9z5coVDh8+zOLFi4tuSapITOeLWFpaWlJvb2/RbUjXLFq0iAsXLrBkyRLOnj3LV7/6Vc6cOcOCBQv4+OOPi25PAiAi3kkptdxsPfcApByeeOIJAE6fPs3IyAinT5++ri7NJAaAlMO+ffuoq6ujuroagOrqaurq6ti3b1/BnUn5GQBSDv39/cybN48DBw4wNDTEgQMHmDdvnrOBakYyAKScNm/eTGtrK9XV1bS2trJ58+aiW5IqYgBIOe3YsYOenh6Gh4fp6elhx44dRbckVcS5gKQcGhsbOXnyJN/85jev1SKCxsbGAruSKuMegJRDRFybBA64Ninc1buDSTOJewBSDidOnOD+++9naGiIvr4+7r33XmpqavjlL39ZdGtSbgaAlNNPf/rT667+/eijj2hoaCiwI6kyBoCU09e//nVOnTrFpUuXqK2t5Y477ii6JakiBoCUw8KFCzl27Ni115cuXeLYsWPeEUwzkoPAUg43mvbZ6aA1ExkAUg5Xp32uqam57tnpoDUTGQBSBYaGhq57lmYiA0CqwNXz/j3/XzOZASBV4Op9NKbz/TSkmzEAJKmkbhoAEdEVEWcj4vCY2sKIeCsiPsieF2T1iIjvR8TRiHgvIr42ZpvV2fofRMTqW/NzJEnjNZ49gB8Dj3ym9jxwMKW0DDiYvQZYCSzLHuuBl2A0MIAXgW8ADwAvXg0NSVIxbhoAKaWfAec/U34MeCVbfgVYNaa+J436BTA/Iu4AHgbeSimdTyldAN7i86EiSZpClY4BLEkpncqWTwNLsuU7gRNj1uvPajeqS5IKMuFB4DR6GsSknQoREesjojcies+dOzdZHytJ+oxKA+BMdmiH7PlsVj8J3DVmvcasdqP656SUXk4ptaSUWpxhUZJunUoDYD9w9Uye1cBPxtS/m50N9CDwu+xQ0QHgoYhYkA3+PpTVJEkFuelsoBHRDfwPYHFE9DN6Ns/fAK9FxDrgOPCtbPU3gEeBo8BFYA1ASul8RPw18Ha23l+llD47sCxJmkIxna9kbGlpSb29vUW3IV3zZVM/TOf/l1QuEfFOSqnlZut5JbAklZQBIEklZQBIUkkZAJJUUgaAJJWUASBJJWUASFJJGQCSVFIGgCSVlAEgSSVlAEhSSRkAklRSBoAklZQBIEklZQBIUkkZAJJUUgaAJJWUASBJJWUASFJJGQCSVFIGgCSVlAEgSSVlAEhSSRkAklRSBoAklZQBIEklZQBIUkkZAJJUUgaAJJWUASBJJWUASFJJTSgAIuJYRPwqIt6NiN6stjAi3oqID7LnBVk9IuL7EXE0It6LiK9Nxg+QJFVmMvYAWlNK96WUWrLXzwMHU0rLgIPZa4CVwLLssR54aRK+W5JUoVtxCOgx4JVs+RVg1Zj6njTqF8D8iLjjFny/lFtEjOsx0c+QppOJBkACfhoR70TE+qy2JKV0Kls+DSzJlu8ETozZtj+rSYVLKY3rMdHPkKaTORPcfkVK6WREfBV4KyL+39g3U0opInL9V58FyXqApUuXTrA9SdKNTGgPIKV0Mns+C/wL8ABw5uqhnez5bLb6SeCuMZs3ZrXPfubLKaWWlFJLQ0PDRNqTJt2N/or3r3vNRBUHQETUR8RXri4DDwGHgf3A6my11cBPsuX9wHezs4EeBH435lCRNGOMPZzjoR3NZBM5BLQE+JdsYGsOsDel9G8R8TbwWkSsA44D38rWfwN4FDgKXATWTOC7JUkTVHEApJR+A/z3L6h/DPzPL6gn4KlKv0+SNLm8EliSSsoAkKSSMgAkqaQMAEkqKQNAkkrKAJCkkjIAJKmkDABJKikDQJJKygCQpJIyACSppAwASSqpid4QRpqWFi5cyIULF27599zq2zwuWLCA8+fP39LvUHkZAJqVLly4MCvm6fc+wrqVPAQkSSVlAEhSSRkAklRSBoAklZQBIEklZQBIUkl5GqhmpfTiH8H35hXdxoSlF/+o6BY0ixkAmpXi//x+1lwHkL5XdBearTwEJEklZQBIUkl5CEiz1myYRmHBggVFt6BZzADQrDQVx/8jYlaMM6i8PAQkSSVlAEhSSRkAklRSBoAklZQBIEklNeUBEBGPRMSvI+JoRDw/1d8vSRo1pQEQEVXAD4CVwHKgLSKWT2UPkqRRU70H8ABwNKX0m5TSEPAq8NgU9yBJYuovBLsTODHmdT/wjbErRMR6YD3A0qVLp64zlVqlVw3n3c4LxzSdTLtB4JTSyymllpRSS0NDQ9HtqCRSSlPykKaTqQ6Ak8BdY143ZjVJ0hSb6gB4G1gWEfdERA3wOLB/inuQJDHFYwAppcsRsRE4AFQBXSml96eyB0nSqCmfDTSl9AbwxlR/ryTpetNuEFiSNDUMAEkqKQNAkkrKAJCkkorpfHFKRJwDjhfdh3QDi4GPim5C+gL/LaV00ytpp3UASNNZRPSmlFqK7kOqlIeAJKmkDABJKikDQKrcy0U3IE2EYwCSVFLuAUhSSRkAUk4R0RURZyPicNG9SBNhAEj5/Rh4pOgmpIkyAKScUko/A84X3Yc0UQaAJJWUASBJJWUASFJJGQCSVFIGgJRTRHQD/w78cUT0R8S6onuSKuGVwJJUUu4BSFJJGQCSVFIGgCSVlAEgSSVlAEhSSRkAklRSBoAklZQBIEkl9f8B9tHQ1XnvY0YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {v: k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = '/mnt/data2/xinsongdu/projects/w2v_resources/wiki_50.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "wv_embedding = loadGloveModel(glove_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embedding['<UNK>'] = np.zeros(VOCAB_DIM)\n",
    "wv_embedding['<PAD>'] = np.zeros(VOCAB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38497 ,  0.80092 ,  0.064106, -0.28355 , -0.026759, -0.34532 ,\n",
       "       -0.64253 , -0.11729 , -0.33257 ,  0.55243 , -0.087813,  0.9035  ,\n",
       "        0.47102 ,  0.56657 ,  0.6985  , -0.35229 , -0.86542 ,  0.90573 ,\n",
       "        0.03576 , -0.071705, -0.12327 ,  0.54923 ,  0.47005 ,  0.35572 ,\n",
       "        1.2611  , -0.67581 , -0.94983 ,  0.68666 ,  0.3871  , -1.3492  ,\n",
       "        0.63512 ,  0.46416 , -0.48814 ,  0.83827 , -0.9246  , -0.33722 ,\n",
       "        0.53741 , -1.0616  , -0.081403, -0.67111 ,  0.30923 , -0.3923  ,\n",
       "       -0.55002 , -0.68827 ,  0.58049 , -0.11626 ,  0.013139, -0.57654 ,\n",
       "        0.048833,  0.67204 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_embedding['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corpus = []\n",
    "for sent in X:\n",
    "    X_corpus.append(' '.join([id_to_word[i] for i in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(sent.split()) for sent in X_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_transformation(word2vec):\n",
    "    from collections import defaultdict\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    idx2tok = []\n",
    "    idx2vec = []\n",
    "    \n",
    "    for i, tok in enumerate(list(word2vec.keys())):\n",
    "        tok2idx[tok] = i\n",
    "        idx2tok.append(tok)\n",
    "        idx2vec.append(word2vec[tok])\n",
    "    return tok2idx, idx2tok, idx2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx, idx2token, idx2vector = embedding_transformation(wv_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2vector = np.array(idx2vector)\n",
    "idx2vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenized_sents(corpus, max_len):\n",
    "    data_x = []\n",
    "    for i, sent in enumerate(corpus):\n",
    "        data_x.append([])\n",
    "        tokenized_sent = tknzr.tokenize(sent)\n",
    "        pad_len = max_len - len(tokenized_sent)\n",
    "#        print(pad_len)\n",
    "        for j in range(max_len):\n",
    "            if j < pad_len:\n",
    "                data_x[i].append(token2idx['<PAD>'])\n",
    "            else:\n",
    "#                for k in sent.split():\n",
    "                if tokenized_sent[j-max_len] in wv_embedding:\n",
    "                    data_x[i].append(token2idx[tokenized_sent[j-max_len]])\n",
    "                else:\n",
    "                    data_x[i].append(token2idx['<UNK>'])\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = prepare_tokenized_sents(X_corpus, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function of disconnecting input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2494"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disconnect_input(input_idx, window_size):\n",
    "    adjusted_input = []\n",
    "    n_windows = seq_len - window_size + 1\n",
    "    for i in range(len(input_idx)):\n",
    "        adjusted_input.append([])\n",
    "        for j in range(n_windows):\n",
    "            adjusted_input[i].append(input_idx[i][j:(j + window_size)])\n",
    "    return n_windows, adjusted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_windows, adjusted_data_x = disconnect_input(data_x, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_data_x = adjusted_data_x[5000:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_data_y = data_y[5000:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(selected_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(selected_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, batch_index, input_x, input_y):\n",
    "    input_x = np.array(input_x)\n",
    "    input_y = np.array(input_y)\n",
    "    if (batch_index+1)*batch_size < len(input_x):\n",
    "        return input_x[batch_index*batch_size: (batch_index+1)*batch_size], input_y[batch_index*batch_size: (batch_index+1)*batch_size]\n",
    "    else:\n",
    "        select = random.sample(range(batch_index * batch_size), (batch_index+1)*batch_size - len(input_x))\n",
    "        return np.concatenate([input_x[batch_index*batch_size: len(input_x)], input_x[select]]), np.concatenate([input_y[batch_index*batch_size: len(input_y)], input_y[select]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(adjusted_data_x, data_y, test_size=0.2, random_state=None, stratify = data_y, shuffle = True)\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=None, stratify = train_y, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "val_x = np.array(val_x)\n",
    "test_x = np.array(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 36, 15)\n",
      "(32000,)\n",
      "(8000, 36, 15)\n",
      "(8000,)\n",
      "(10000, 36, 15)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRNN():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def declare_placeholders(self):\n",
    "    self.input_batch = tf.placeholder(dtype=tf.int32, shape=[None, None, None], name='input_batch')\n",
    "    self.ground_truth_labels = tf.placeholder(dtype=tf.int32, shape=[None, None], name='ground_truth_labels')\n",
    "    \n",
    "#    self.lengths = tf.placeholder(dtype=tf.int32, shape=[None], name='lengths')\n",
    "    self.dropout_ph = tf.placeholder_with_default(tf.cast(0.5, tf.float32), shape=[])\n",
    "    \n",
    "    self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.__declare_placeholders = classmethod(declare_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_layers(self, vocabulary_size, n_hidden_rnn, window_size):\n",
    "    \n",
    "#    window_size = 15\n",
    "    \n",
    "    n_windows = seq_len - window_size + 1\n",
    "    \n",
    "    initial_embedding_matrix = idx2vector    \n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        embedding_matrix_variable = tf.Variable(initial_embedding_matrix, dtype = tf.float32, name = 'embedding_matrix_variable', trainable = False)\n",
    "#        self.embeddings = tf.nn.embedding_lookup(params = embedding_matrix_variable, ids = self.input_batch)\n",
    "    \n",
    "    self.unpooled_outputs = []\n",
    "    for i in range(n_windows): \n",
    "#        print(i)\n",
    "        with tf.name_scope(\"gru_block_%s\"%i):\n",
    "            embeddings = tf.nn.embedding_lookup(params = embedding_matrix_variable, ids = self.input_batch[:, i], \n",
    "                                                     name = 'embedding_output')\n",
    "            gru_cell = tf.nn.rnn_cell.DropoutWrapper(cell = tf.nn.rnn_cell.GRUCell(num_units = n_hidden_rnn), \n",
    "                                                         input_keep_prob = self.dropout_ph,\n",
    "                                                         output_keep_prob = self.dropout_ph,\n",
    "                                                         state_keep_prob = self.dropout_ph)\n",
    "            self.rnn_output, _ = tf.nn.dynamic_rnn(cell = gru_cell,\n",
    "                                                   dtype = tf.float32,\n",
    "                                                   inputs = embeddings,\n",
    "                                                   scope = \"rnn_%s\"%i)\n",
    "\n",
    "            self.extracted_rnn_output = tf.layers.batch_normalization(self.rnn_output[:,-1], axis = -1)\n",
    "            self.unpooled_outputs.append(tf.layers.dense(self.extracted_rnn_output, 64, activation=None))\n",
    "\n",
    "    self.pooled_outputs = tf.layers.max_pooling1d(inputs = tf.transpose(self.unpooled_outputs, [1, 0, 2]),\n",
    "                                                  pool_size = n_windows,\n",
    "                                                  strides = 1,\n",
    "                                                  padding = 'valid')\n",
    "\n",
    "    self.logits = tf.layers.dense(self.pooled_outputs, 2, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.__build_layers = classmethod(build_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(self):\n",
    "    softmax_output = tf.nn.softmax(logits = self.logits)\n",
    "    self.predictions = tf.argmax(input = softmax_output, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.__compute_predictions = classmethod(compute_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self):\n",
    "    ground_truth_labels_one_hot = tf.one_hot(self.ground_truth_labels, 2)\n",
    "    self.loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels = ground_truth_labels_one_hot, logits = self.logits)\n",
    "    self.loss = tf.reduce_mean(self.loss_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.__compute_loss = classmethod(compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(self):\n",
    "    self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate_ph)\n",
    "    self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
    "    clip_norm = tf.cast(1.0, tf.float32)\n",
    "    self.grads_and_vars = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in self.grads_and_vars]\n",
    "    self.train_op = self.optimizer.apply_gradients(self.grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.__perform_optimization = classmethod(perform_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(self, vocabulary_size, n_hidden_rnn, window_size):\n",
    "    self.__declare_placeholders()\n",
    "    self.__build_layers(vocabulary_size, n_hidden_rnn, window_size)\n",
    "    self.__compute_predictions()\n",
    "    self.__compute_loss()\n",
    "    self.__perform_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.__init__ = classmethod(init_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network and do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(self, session, x_batch, y_batch, learning_rate, dropout_keep_probability):\n",
    "    feed_dict = {self.input_batch: x_batch,\n",
    "                 self.ground_truth_labels: y_batch,\n",
    "                 self.learning_rate_ph: learning_rate,\n",
    "                 self.dropout_ph: dropout_keep_probability}\n",
    "    \n",
    "    session.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.train_on_batch = classmethod(train_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_batch(self, session, x_batch):\n",
    "    \n",
    "    feed_dict = {self.input_batch: x_batch}\n",
    "    predictions = session.run(self.predictions, feed_dict = feed_dict)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRNN.predict_for_batch = classmethod(predict_for_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = DRNN(vocabulary_size = len(token2idx),\n",
    "                   n_hidden_rnn = 64, window_size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "batch_size = 16\n",
    "n_epochs = 32\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 1.2\n",
    "dropout_keep_probability = 0.7\n",
    "#lengths = max_len\n",
    "n_batches = math.ceil(len(train_x)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, session, data, label):#, lengths):\n",
    "    from sklearn.metrics import precision_recall_fscore_support as prf\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "    y_pred = model.predict_for_batch(sess, data)#, lengths)\n",
    "    y_true = label\n",
    "    precision, recall, f_score, _ = prf(y_true = y_true, y_pred = y_pred, average = 'macro')\n",
    "    auroc = roc_auc_score(y_true = y_true, y_score = y_pred, average = 'macro')\n",
    "    acc = accuracy_score(y_true = y_true, y_pred = y_pred)\n",
    "    print('precision: ', precision, 'recall: ', recall, 'f-score: ', f_score, 'AUROC: ', auroc, 'accuracy: ', acc)\n",
    "#    return nn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "-------------------- Epoch 1 of 32 --------------------\n",
      "Train data evaluation:\n",
      "precision:  0.5087681855156749 recall:  0.501375 f-score:  0.3681926955259185 AUROC:  0.501375 accuracy:  0.501375\n",
      "Validation data evaluation:\n",
      "precision:  0.5103624010639795 recall:  0.501625 f-score:  0.3685094703088085 AUROC:  0.501625 accuracy:  0.501625\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "-------------------- Epoch 2 of 32 --------------------\n",
      "Train data evaluation:\n",
      "precision:  0.5468847371296512 recall:  0.54471875 f-score:  0.539399024186253 AUROC:  0.54471875 accuracy:  0.54471875\n",
      "Validation data evaluation:\n",
      "precision:  0.545051026214955 recall:  0.5427500000000001 f-score:  0.5368358570587837 AUROC:  0.5427500000000001 accuracy:  0.54275\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "-------------------- Epoch 3 of 32 --------------------\n",
      "Train data evaluation:\n",
      "precision:  0.5761263750326491 recall:  0.576125 f-score:  0.57612308593081 AUROC:  0.5761249999999999 accuracy:  0.576125\n",
      "Validation data evaluation:\n",
      "precision:  0.5793756002779771 recall:  0.579375 f-score:  0.5793742047543559 AUROC:  0.5793750000000001 accuracy:  0.579375\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "-------------------- Epoch 4 of 32 --------------------\n",
      "Train data evaluation:\n",
      "precision:  0.5921625915463589 recall:  0.58940625 f-score:  0.5863131778654751 AUROC:  0.58940625 accuracy:  0.58940625\n",
      "Validation data evaluation:\n",
      "precision:  0.595445585641664 recall:  0.5920000000000001 f-score:  0.5882842654961022 AUROC:  0.5920000000000001 accuracy:  0.592\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "-------------------- Epoch 5 of 32 --------------------\n",
      "Train data evaluation:\n",
      "precision:  0.6009554413344759 recall:  0.5901562499999999 f-score:  0.5788948585017841 AUROC:  0.59015625 accuracy:  0.59015625\n",
      "Validation data evaluation:\n",
      "precision:  0.5778881797441826 recall:  0.5698749999999999 f-score:  0.5585200742259775 AUROC:  0.569875 accuracy:  0.569875\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "-------------------- Epoch 6 of 32 --------------------\n",
      "Train data evaluation:\n",
      "precision:  0.6012163229294591 recall:  0.6011875 f-score:  0.601159105955883 AUROC:  0.6011875 accuracy:  0.6011875\n",
      "Validation data evaluation:\n",
      "precision:  0.6060892210348904 recall:  0.6060000000000001 f-score:  0.6059171440795428 AUROC:  0.6060000000000001 accuracy:  0.606\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "-------------------- Epoch 7 of 32 --------------------\n",
      "Train data evaluation:\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):\n",
    "    # For each epoch evaluate the model on train and validation data\n",
    "    print('-' * 20 + ' Epoch {} '.format(epoch+1) + 'of {} '.format(n_epochs) + '-' * 20)\n",
    "    print('Train data evaluation:')\n",
    "    evaluation(model, sess, train_x, train_y)\n",
    "    print('Validation data evaluation:')\n",
    "    evaluation(model, sess, val_x, val_y)\n",
    "    \n",
    "    # Train the model\n",
    "    for batch_index in range(n_batches):\n",
    "#        print(\".\",end=\"\")\n",
    "        if (batch_index*100/n_batches)%10 == 0:\n",
    "            print(batch_index/n_batches)\n",
    "        dt = batch_generator(batch_size, batch_index, train_x, train_y)\n",
    "        feed_dict = {model.input_batch: dt[0],\n",
    "                 model.ground_truth_labels: dt[1].reshape(batch_size,1),\n",
    "                 model.learning_rate_ph: learning_rate,\n",
    "                 model.dropout_ph: dropout_keep_probability}\n",
    "        model.train_on_batch(sess, dt[0], dt[1].reshape(batch_size, 1), learning_rate, dropout_keep_probability)\n",
    "    \n",
    "print('...training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
